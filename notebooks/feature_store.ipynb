{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffde0a55",
   "metadata": {},
   "source": [
    "# Feature Store and Data Versioning with Delta Lake\n",
    "\n",
    "This notebook demonstrates building a simple feature store using Delta Lake for data versioning. It covers ingesting raw CSV data, computing derived features, and using time travel queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fa127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Configure Spark session with Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FeatureStore\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session with Delta Lake configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08972800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest raw CSV data into Delta table\n",
    "raw_data_path = \"../data/raw_data.csv\"\n",
    "delta_raw_path = \"../delta/raw_data\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(raw_data_path, header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "# Write to Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_raw_path)\n",
    "print(f\"Raw data ingested into Delta table at {delta_raw_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derived features\n",
    "# Read from Delta table\n",
    "raw_df = spark.read.format(\"delta\").load(delta_raw_path)\n",
    "\n",
    "# Compute aggregations: total sales per product, average sales per month\n",
    "from pyspark.sql.functions import sum, avg, month, year\n",
    "\n",
    "features_df = raw_df.groupBy(\"product\").agg(\n",
    "    sum(\"sales\").alias(\"total_sales\"),\n",
    "    avg(\"sales\").alias(\"avg_sales\")\n",
    ").withColumn(\"month\", month(\"date\")).withColumn(\"year\", year(\"date\"))\n",
    "\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ce899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write features to versioned Delta table\n",
    "delta_features_path = \"../delta/features\"\n",
    "\n",
    "features_df.write.format(\"delta\").mode(\"overwrite\").save(delta_features_path)\n",
    "print(f\"Features written to Delta table at {delta_features_path}\")\n",
    "\n",
    "# To demonstrate versioning, let's add more data and write again\n",
    "# Simulate new data\n",
    "new_data = spark.createDataFrame([\n",
    "    (\"2023-03-01\", \"ProductA\", 140),\n",
    "    (\"2023-03-02\", \"ProductB\", 280)\n",
    "], [\"date\", \"product\", \"sales\"])\n",
    "\n",
    "# Append to raw\n",
    "new_data.write.format(\"delta\").mode(\"append\").save(delta_raw_path)\n",
    "\n",
    "# Recompute features\n",
    "raw_df_updated = spark.read.format(\"delta\").load(delta_raw_path)\n",
    "features_df_updated = raw_df_updated.groupBy(\"product\").agg(\n",
    "    sum(\"sales\").alias(\"total_sales\"),\n",
    "    avg(\"sales\").alias(\"avg_sales\")\n",
    ")\n",
    "\n",
    "features_df_updated.write.format(\"delta\").mode(\"overwrite\").save(delta_features_path)\n",
    "print(\"Features updated and versioned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate time travel queries\n",
    "# Get history of the features table\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_features_path)\n",
    "history = delta_table.history()\n",
    "history.show()\n",
    "\n",
    "# Query version 0 (initial features)\n",
    "version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_features_path)\n",
    "print(\"Version 0:\")\n",
    "version_0.show()\n",
    "\n",
    "# Query version 1 (updated features)\n",
    "version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_features_path)\n",
    "print(\"Version 1:\")\n",
    "version_1.show()\n",
    "\n",
    "# Query latest\n",
    "latest = spark.read.format(\"delta\").load(delta_features_path)\n",
    "print(\"Latest:\")\n",
    "latest.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
